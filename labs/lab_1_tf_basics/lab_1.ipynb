{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Grade: (2.9+3+2.7)/3 = 2.87/3 . Good job!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: TensorFlow Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 01:15:21.233390: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Tensors\n",
    "If you want to review any of this material, look over https://docs.scipy.org/doc/numpy-1.15.0/user/quickstart.html and https://www.tensorflow.org/guide/tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Tensor values, shapes, rank, and axes\n",
    "Make tensor values by hand (e.g. `x = np.array([[1, 2, 3], [4, 5, 6]])`) of the following shapes:\n",
    " * a: (2, 2)\n",
    " * b: (3)\n",
    " * c: (3, 1)\n",
    " * d: (1, 3)\n",
    " * e: ()\n",
    " * f: (1)\n",
    " * g: (2, 2, 2)\n",
    " * h: (2, 3, 1, 2)\n",
    " \n",
    " For each, put its tensor rank and total number of elements in a comment.\n",
    " Yes, this is pretty boring, but it's also short and it's really important to understand what tensors of different shapes look like and how shapes, rank, and axes interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\na: rank = 2, num_elements = 4\\nb: rank = 1, num_elements = 3\\nc: rank = 2, num_elements = 3\\nd: rank = 2, num_elements = 3\\ne: rank = 0, num_elements = 1\\nf: rank = 1, num_elements = 1\\ng: rank = 3, num_elements = 8\\nh: rank = 4, num_elements = 12\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "b = np.array([1, 2, 3])\n",
    "c = np.array([[1], \n",
    "              [2], \n",
    "              [3]])\n",
    "d = np.array([[1, 2, 3]])\n",
    "e = np.array(1)\n",
    "f = np.array([1])\n",
    "g = np.array([[[1, 2], \n",
    "               [3, 4]], \n",
    "              [[5, 6],\n",
    "               [7, 8]]])\n",
    "h = np.array([[[[1, 2]], [[3, 4]], [[5, 6]]], [[[7, 8]], [[9, 10]], [[11, 12]]]])\n",
    "\n",
    "\"\"\"\n",
    "a: rank = 2, num_elements = 4\n",
    "b: rank = 1, num_elements = 3\n",
    "c: rank = 2, num_elements = 3\n",
    "d: rank = 2, num_elements = 3\n",
    "e: rank = 0, num_elements = 1\n",
    "f: rank = 1, num_elements = 1\n",
    "g: rank = 3, num_elements = 8\n",
    "h: rank = 4, num_elements = 12\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "-0.1: For part e, it should have just been an empty array with rank 0 and 0 elements. You had the right idea for the array e that you made though.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Slices and reductions\n",
    "Use slicing or `tf.reduce_mean`, `tf.reduce_sum`, and `tf.reduce_any` on the tensors defined below to print:\n",
    " * The (1-2-3)-st element of `a`\n",
    " * The first column of `b`\n",
    " * The shape-(2, 3, 2) tensor obtained by selecting the second and third elements of the third axis of `a`\n",
    " * The sum of all values in `b`\n",
    " * The 2-vector containing means of each row of `b` \n",
    " * The (1, 3) tensor containing, for each column in `c`, whether that column contains any `True` values\n",
    " \n",
    "Each statement should take the form \n",
    "```\n",
    "tf.print(something[...])\n",
    "```\n",
    "or \n",
    "```\n",
    "tf.print(tf.reduce_something(...))\n",
    "```\n",
    "Follow each with a comment stating the shape of the output.\n",
    "For a rank-2 tensor, the first index specifies row and the second specifies column.\n",
    "Make sure to pay attention to the `axis` and `keepdims` arguments of the `reduce` functions. Also, recall that TensorFlow is **0-indexed**.\n",
    " \n",
    " \n",
    "For this problem, I'll set up the name scope, but for all future problems you'll need to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(np.ones((2, 3, 4))) # Tensor of ones with shape (2, 3, 4)\n",
    "b = tf.constant([[1., 2.], \n",
    "                 [3., 4.]]) # Tensor of the matrix [1 2; 3 4] with shape (2, 2)\n",
    "c = tf.constant([[True, True, False],\n",
    "                 [False, True, False]]) # Binary tensor with shape (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1 3]\n",
      "[[[1 1]\n",
      "  [1 1]\n",
      "  [1 1]]\n",
      "\n",
      " [[1 1]\n",
      "  [1 1]\n",
      "  [1 1]]]\n",
      "10\n",
      "[1.5 3.5]\n",
      "[1 1 0]\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('slices_and_reductions'):\n",
    "    tf.print(a[1, 2, 3])            # ()\n",
    "    tf.print(b[:, 0])               # (2)\n",
    "    tf.print(a[:, :, 1:3])          # (2, 3, 2)\n",
    "    tf.print(tf.reduce_sum(b))      # ()\n",
    "    tf.print(tf.reduce_mean(b, 1))  # (2)\n",
    "    tf.print(tf.reduce_any(c, 0))   # (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Transposition and reshaping\n",
    "Use `tf.transpose` to print:\n",
    " * `b` with its rows and columns swapped\n",
    " * `a` with its second and third axes swapped; comment its shape\n",
    " \n",
    "Use `tf.reshape` to print:\n",
    " * The values of `b` in a tensor with shape (1, 4)\n",
    " * The values of `b` in a tensor with shape (4, 1)\n",
    " \n",
    "Do this all inside the name scope \"transposition_and_reshaping\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3]\n",
      " [2 4]]\n",
      "[[[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n",
      "[[1 2 3 4]]\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('transposition_and_reshaping'):\n",
    "  tf.print(tf.transpose(b))\n",
    "  tf.print(tf.transpose(a, [0, 2, 1])) # (2, 4, 3)\n",
    "\n",
    "  tf.print(tf.reshape(b, (1, 4)))\n",
    "  tf.print(tf.reshape(b, (4, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Computing with Operations and Graphs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: The dot product (as a sum of scalar products)\n",
    "Write a function `dot_sum()` that takes in two rank-1 tensors `a` and `b` of equal shape and returns a tensor that holds their dot product, $$\\text{result} = a \\cdot b = \\sum_{i = 1}^{\\dim{a}} a_i \\cdot b_i $$\n",
    "\n",
    "The computation should first multiply the elements in $a$ and $b$ into a vector $a \\odot b$ (the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) of $a$ and $b$), then sum across the vector to produce a scalar. \n",
    "Your implementation should be _vectorized_: it should not explicitly use the shape of an input tensor or do any looping.\n",
    "The tensor output by your function must be rank-0.\n",
    "\n",
    "The entire computation should use the name scope \"dot_sum\" and the tensor you return should have the name \"result\".\n",
    "\n",
    "TensorFlow operations to look at:\n",
    " * `tf.multiply` (or equivalently, the binary operation *)\n",
    " * `tf.reduce_sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_sum(a, b):\n",
    "    '''\n",
    "    Given rank-1 tensors a and b with equal shapes, return the dot product \n",
    "    of a and b as a rank-0 tensor computed via Hadamard product.\n",
    "    '''\n",
    "    with tf.name_scope('dot_sum'):\n",
    "        result = tf.reduce_sum(tf.multiply(a, b))\n",
    "        return tf.Variable(result, name='result', shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dot_sum/result:0' shape=() dtype=int64, numpy=14>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_sum(np.array([1,2,3]), np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Good! You didn't have to return a tf.variable, and could have just returned the result directly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: The dot product (as matrix multiplication)\n",
    "Write a function `dot_multiply()` that takes in two rank-1 tensors `a` and `b` of equal shape and returns a tensor that holds their dot product, $$\\text{result} = a \\cdot b = a^T b $$\n",
    "\n",
    "The computation should use `tf.matmul` to perform the multiplication, which expects that your input tensors have rank of at least two (they should be matrices, not vectors).\n",
    "Since your input vectors are rank-1, this means you'll need to use `tf.expand_dims` with `axis=-1` to add a \"dummy dimension\".\n",
    "This is a subtle but important point: your vectors start with shape [n], but matrix multiplication is only defined for matrices with shapes [1, n] and [n, 1].\n",
    "Depending on how you do it, you will probably get a rank-2 tensor with a shape like [1, 1].\n",
    "You must return a rank-0 tensor, so use `tf.squeeze` to eliminate dummy dimensions.\n",
    "\n",
    "The entire computation should use the name scope \"dot_multiply\" and the tensor you return should have the name \"result\".\n",
    "This will not collide with the previous \"result\" tensor because of name scoping.\n",
    "(If it did, it would be renamed to \"result_0\" in the graph)\n",
    "\n",
    "TensorFlow operations to look at:\n",
    " * `tf.matmul`\n",
    " * `tf.transpose`\n",
    " * `tf.expand_dims`\n",
    " * `tf.squeeze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_multiply(a, b):\n",
    "    '''\n",
    "    Given rank-1 tensors a and b with equal shapes, return the dot product \n",
    "    of a and b as a rank-0 tensor computed via matrix multiplication.\n",
    "    '''\n",
    "    with tf.name_scope('dot_multiply'):\n",
    "        a = tf.expand_dims(a, axis = -1)\n",
    "        b = tf.expand_dims(b, axis = -1)\n",
    "        result = tf.matmul(tf.transpose(a), b)\n",
    "        result = tf.squeeze(result)\n",
    "        return tf.Variable(result, name='result', shape=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: A single ReLU unit\n",
    "The \"default\" activation function for modern neural networks is the [rectified linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) (or \"ReLU\"):\n",
    "$$ \\text{relu}(x) = max(0, x). $$\n",
    "\n",
    "In a neural network using ReLU activation, a single unit with $n$ inputs has parameters $w$ (an $n$-vector of weights) and $b$ (a scalar).\n",
    "It computes the function\n",
    "$$ f(x; w, b) = \\text{relu}(w \\cdot x + b). $$\n",
    "\n",
    "Using either `dot_sum` or `dot_multiply`, add these tensors and operations to the default graph:\n",
    "$$\n",
    "\\begin{align}\n",
    "&x: \\space \\text{placeholder} \\\\\n",
    "&w = \\begin{bmatrix}2 & 0.5 & -1\\end{bmatrix} \\\\\n",
    "&b = 0.3 \\\\\n",
    "&\\text{state} = w \\cdot x + b \\\\\n",
    "&\\text{activation} = \\max(\\text{state}, 0)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "`x` should have shape [3] and dtype `tf.float32`, and all tensors should be named, under the name scope \"ReLU\".\n",
    "This includes the tensors created through your dot product function, but do not change your implementation to add to the name! Then wrap all of this in a function called `relu` that takes in one argument to initialize the `tf.Variable` `x` object, then returns `activation`.\n",
    "\n",
    "Then, print `relu` for:\n",
    " * $x = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$\n",
    " * $x = \\begin{bmatrix} -1 & 2 & 0 \\end{bmatrix}$\n",
    " * $x = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}$\n",
    " * $x = \\begin{bmatrix} 0 & 0 & 0 \\end{bmatrix}$\n",
    "\n",
    "TensorFlow operations to look at:\n",
    " * tf.constant\n",
    " * tf.Variable\n",
    " * tf.add\n",
    " * tf.maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  with tf.name_scope('ReLU'):\n",
    "    w = tf.constant(np.array([2, .5, -1]), name=\"weight\")\n",
    "    b = tf.constant(.3, name=\"b\", dtype=tf.double)\n",
    "    state = tf.add(dot_sum(w, x), b, name=\"state operation\")\n",
    "    activation = tf.Variable(tf.maximum(state, 0), name=\"activation\")\n",
    "  \n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1:  1.8\n",
      "x2:  0\n",
      "x3:  2.3\n",
      "x4:  0.3\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([1, 1, 1])\n",
    "x2 = np.array([-1, 2, 0])\n",
    "x3 = np.array([1, 0, 0])\n",
    "x4 = np.array([0, 0, 0])\n",
    "\n",
    "tf.print('x1: ', relu(x1))\n",
    "tf.print('x2: ', relu(x2))\n",
    "tf.print('x3: ', relu(x3))\n",
    "tf.print('x4: ', relu(x4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside on activation functions\n",
    "\n",
    "One way to derive feedforward neural networks is to begin by saying \"I'd like to do a simple (linear) transformation on my input features to make them easier to model, then use a simple model (e.g. linear regression) that instead uses the transformed features.\"\n",
    "Doing this means your total model is $y = ABx + b$ where $B$ is the matrix multiplying an input point $x$ into a new representation and $A$ is the matrix parameterizing the linear regression.\n",
    "\n",
    "But, $AB$ is just another matrix, and so by adding a representation you have not made your model more powerful; instead if you'd \"twisted\" the input space after appyling B, the overall map would be nonlinear and the composite model would have greater representation power.\n",
    "Activation functions perform this \"twisting\".\n",
    "Deep neural networks come from the observation that it'd be easier to get a good representation (top layer) if it was based on a lower-level representation (early layers).\n",
    "\n",
    "Here's a great article explaining the geometric interpretation of activation functions: https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/.\n",
    "The general idea is that neural networks can learn parameters that use the \"twists\" such that the entire network deforms space so that the manifold defined by your input data is simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing a function with gradient descent\n",
    "Minimize the scalar function $f(x) = (x-1)(2x-2)(x-3)(x-4)$, plotted below, using gradient descent.\n",
    "It has a local minimum near $x = 1$ and a global minimum near $x = 3.5$.\n",
    "\n",
    "![f(x)](./images/plot_f.png)\n",
    "\n",
    "The steps to build the graph are:\n",
    " 1. Use `tf.Variable` to create a variable named `x` that uses a `np.random.uniform` on the range [-1, 5] to initialize.\n",
    " 2. Make a `tf.optimizers.SGD` named \"optimizer\" with a learning rate of 0.01.\n",
    " 3. Make a function `optimize` that takes in an optimizer as an argument and represents each step of the training loop.\n",
    " 4. Create a `tf.GradientTape` object and make a tensor `y` that represents $f(x)$ given a value of `x` under it.\n",
    " 5. Get the gradients of `y` from the `tf.GradientTape` and apply them to the optimizer.\n",
    " \n",
    "Remember steps 4 and 5 go inside the `optimize` function!\n",
    "The whole subgraph for this problem should go under a name scope of \"minimize_f\", and operations to compute `y` should have an additional name scope of \"compute_f\". \n",
    "\n",
    "In a comment, rewrite the `optimize` function using the `minimize` function on the optimizer instead of getting the gradients and applying them. Is `tf.GradientTape` is necessary? You do not need to worry about `tf.name_scope` for this.\n",
    "\n",
    "Then, the steps to minimize the function once are:\n",
    " 1. Print the initial values of `x` and `y`.\n",
    " 2. Run `minimize` 1000 times.\n",
    " 3. Print the final values of `x` and `y`.\n",
    " \n",
    "Minimize the function a few times. If you did it right, you'll find that in each run the optimizer finds one of two minima. Running minimization a few times, you should see it find both eventually. What determines which minimum is found? Answer in the markdown box below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('minimize_f'):\n",
    "  def f(x):\n",
    "    return (x - 1) * (2*x - 2) * (x - 3) * (x - 4)\n",
    "  \n",
    "  x = tf.Variable(np.random.uniform(-1, 5), name='x')\n",
    "  optimizer = tf.optimizers.legacy.SGD(learning_rate=.01, name='optimizer')   # using legacy.SGD instead of .SGD since legacy.SGD is (currently) faster for M1 Macs\n",
    "  \n",
    "  def optimize(optimizer):\n",
    "    with tf.name_scope('compute_f'):\n",
    "      with tf.GradientTape() as g:\n",
    "        y = f(x)\n",
    "      args = [x]\n",
    "      grads = g.gradient(y, args)\n",
    "      optimizer.apply_gradients(zip(grads, args))\n",
    "  \n",
    "  \"\"\" \n",
    "  def optimize(optimizer):\n",
    "    with tf.name_scope('compute_f'):\n",
    "      with tf.GradientTape() as g:\n",
    "        y = lambda: f(x)\n",
    "      args = [x]\n",
    "      optimizer.minimize(y, args)\n",
    "  \"\"\"\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial x:  0.453248322\n",
      "Initial y:  5.40042114\n",
      "Final x:  0.999999881\n",
      "Final y:  1.70530257e-13\n"
     ]
    }
   ],
   "source": [
    "tf.print(f'Initial x: ', x)\n",
    "tf.print(f'Initial y: ', f(x))\n",
    "\n",
    "for _ in range(1000):\n",
    "  optimize(optimizer)\n",
    "\n",
    "tf.print(f'Final x: ', x)\n",
    "tf.print(f'Final y: ', f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "-0.3: Forgot to answer \"what determines which minima is found\"\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
